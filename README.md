## 02456 Deep Learning - Natural Language Processing

***Authors: Julie Maria Petersen & Lise Styve*** <br /> <br />

### Word-Level Language Modelling Using an LSTM Neural Network

Abstract

In this paper, word-level language modelling is considered with a simple objective, which is predicting the next word given the previous words within some text. This Natural Language Processing (NLP) task is approached with supervised learning using a Recurrent Neural Network (RNN) with Long Short-Term Memory (LSTM) cells. The model is sought optimized and regularized using weight decay and dropout. A validation perplexity of 88.2 is achieved on the task specific data set Penn Treebank (PTB). When trained on other data sets, the language model enables to adapt to the style and content of the conditioning text, however the generated sequences are not sufficiently realistic and coherent. This can be due to a continued need for regularization and larger amounts of data.
